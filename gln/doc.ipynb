{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Linear Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the concept of a neuron.\n",
    "\n",
    "Each neuron has the following features:\n",
    "* A set of **weight** vectors. Each of these vectors belongs to one out of a set of contexts that the neuron maintains. These vectors are grouped in an array and accessed by their index.\n",
    "* A **context function** that maps provided side information (in our case the inputs themselves) to an index of the weight matrix. This context function is realised by two parts:\n",
    "    1. A matrix of vectors **v**, randomly sampled from $$N(0, 0.1)$$, that project each input vector onto the number line.\n",
    "    2. A vector of values **b**, again randomly sampled from a Gaussian, that are used to decide which side of the halfspace the given input falls on.\n",
    "\n",
    "Given these features, a neuron\n",
    "1. projects the side information onto the number line for different halfspace hypotheses.\n",
    "2. produces a binary vector from the numbers by checking whether they are greater than a fixed set of values to decide which side of the halfspace the input falls on.\n",
    "3. converts the binary vector to an index to choose which context to use for this particular input.\n",
    "4. multiplies the chosen weight vector with the output from the previous layer to produce the output.\n",
    "5. computes the loss directly by comparing its output to the ground truth and updates the chosen weight vector.\n",
    "\n",
    "Let's implement each of these parts one by one.\n",
    "For this hypothetical neuron, we are going to assume `context_dim = 4` for each neuron, side information dimension to be `side_info_dim = 784` (the length of each data point in the MNIST dataset) and the input dimension to be `input_dim = 128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_dim = 4\n",
    "side_info_dim = 784\n",
    "input_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some random side information and input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_info = np.random.normal(size=(784, 1))\n",
    "previous_layer_output = np.random.normal(size=(128, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the parts of the context function. The projection matrix will have dimensions `(context_dim, side_info_dim)`, the threshold vector will have dimensions `(context_dim, 1)`. Additionally, as mentioned in the paper, the projection vectors will be scaled by their L2 norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.normal(loc=0, scale=0.1, size=(context_dim, side_info_dim))\n",
    "v /= np.linalg.norm(v, ord=2, axis=1, keepdims=True)\n",
    "b = np.random.normal(loc=0, scale=0.1, size=(context_dim, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a side information, we can compute the binary vector that will serve as the index of the context based weight vector as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1]\n [0]\n [0]\n [1]]\n"
    }
   ],
   "source": [
    "binary_context = (v.dot(side_info) > b).astype(np.int)\n",
    "print(binary_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this boolean into an integer index, and in keeping with vectorised implementations, our neurons will also have a converter matrix that holds powers of `2` as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1]\n [2]\n [4]\n [8]]\n"
    }
   ],
   "source": [
    "boolean_converter = np.array([[2 ** i] for i in range(context_dim)])\n",
    "print(boolean_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us a lot when it comes to batched vectorised implementations.\n",
    "We can use these bits to find our context specific weight vector index as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    "current_context = np.squeeze(np.sum(binary_context * boolean_converter, axis=0))\n",
    "print(current_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our given side information, we are going to use the weight vector at index `current_context`.\n",
    "\n",
    "Now let's define the weight vectors that we are going to choose from.\n",
    "Given that our context dimension is `4` and that we derive a 4-bit binary index for each context, we can have `2^4 = 16` different weight vectors to choose from. The neuron will take the selected weight vector and multiply it with the output from the previous layer to produce the output logit. We can straightaway put all the vectors into one matrix and define it as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones(shape=(2 ** context_dim, input_dim)) * (1 / input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have used uniform initialisation of the weights as suggested in the paper.\n",
    "\n",
    "Having determined the specific context to use, we can choose the weight vector and process the input as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.03063838]\n"
    }
   ],
   "source": [
    "output_logit = weights[current_context].dot(previous_layer_output)\n",
    "print(output_logit)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitscrabbleconda997c24ac097a4ece945246645d076769",
   "display_name": "Python 3.8.2 64-bit ('scrabble': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}